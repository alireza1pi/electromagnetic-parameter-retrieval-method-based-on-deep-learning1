{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba358bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model configuration\n",
    "batch_size = 50\n",
    "img_width, img_height, img_num_channels = 32, 32, 3\n",
    "loss_function = sparse_categorical_crossentropy\n",
    "no_classes = 100\n",
    "no_epochs = 100\n",
    "optimizer = Adam()\n",
    "verbosity = 1\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "(input_train, target_train), (input_test, target_test) = cifar10.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b70622",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data sets\n",
    "IRIS_TRAINING = \"iris_training.csv\"\n",
    "IRIS_TEST = \"iris_test.csv\"\n",
    "\n",
    "def main():\n",
    "  # Load datasets.\n",
    "  training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TRAINING,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "  \n",
    "  test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n",
    "      filename=IRIS_TEST,\n",
    "      target_dtype=np.int,\n",
    "      features_dtype=np.float32)\n",
    "\n",
    "  # Specify that all features have real-value data\n",
    "  feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=4)]\n",
    "\n",
    "  # Build 3 layer DNN\n",
    "  classifier = tf.contrib.learn.DNNClassifier(feature_columns=feature_columns,\n",
    "                                              hidden_units=[5,10,5],\n",
    "                                              n_classes=3)\n",
    "  # Define the training inputs\n",
    "  def get_train_inputs():\n",
    "    x = tf.constant(training_set.data)\n",
    "    y = tf.constant(training_set.target)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "  # Fit model.\n",
    "  classifier.fit(input_fn=get_train_inputs, steps=2000)\n",
    "\n",
    "  # Define the test inputs\n",
    "  def get_test_inputs():\n",
    "    x = tf.constant(test_set.data)\n",
    "    y = tf.constant(test_set.target)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "  # Evaluate accuracy.\n",
    "  accuracy_score = classifier.evaluate(input_fn=get_test_inputs,\n",
    "                                       steps=1)[\"accuracy\"]\n",
    "\n",
    "  print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score))\n",
    "\n",
    "  # Classify new flower\n",
    "  def new_samples():\n",
    "    return np.array([[6.4, 2.7, 5.6, 2.1]], dtype=np.float32)\n",
    "\n",
    "  predictions = list(classifier.predict(input_fn=new_samples))\n",
    "\n",
    "  print(\"Predicted class: {}\\n\".format(predictions))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32efe975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BostonDataset(torch.utils.data.Dataset):\n",
    "  '''\n",
    "  Prepare the Boston dataset for regression\n",
    "  '''\n",
    "\n",
    "  def __init__(self, X, y, scale_data=True):\n",
    "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "        \n",
    "      # Apply scaling if necessary\n",
    "        if scale_data:\n",
    "            X = StandardScaler().fit_transform(X)\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65485452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(13, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, 32),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "  \n",
    "  # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    X = data.drop(['epsilon','tan_del','index'],axis=1).values\n",
    "    y=data['epsilon'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad285bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv ('ff.csv',index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00a970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = BostonDataset(X,y)\n",
    "trainloader = torch.utils.data.DataLoader(data, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993c9ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "  \n",
    "  # Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdd648",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(0, 5): # 5 epochs at maximum\n",
    "    \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "              # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "              # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                 (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "\n",
    "  # Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb3b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec28abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176e035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # store the inputs and outputs\n",
    "        self.X = ...\n",
    "        self.y = ...\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc3745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv ('ff.csv',index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89fefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a73cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path)\n",
    "        # store the inputs and outputs\n",
    "        self.X = data.drop(['index','tan_del','epsilon'],axis=1).values\n",
    "        self.y = data.drop(['index','SP_imag','SP_real','frequency','epsilon'],axis=1).values\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "\n",
    "    train, test =  data.drop(['tan_del','epsilon','index'],axis=1).values, dataset.drop(['frequency','SP_real','SP_imag','index','epsilon'],axis=1).values\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "path = \"ff.csv\"\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "\n",
    "model = MLP(3)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68296c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv'\n",
    "train_dl, test_dl = prepare_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb278bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl.dataset.dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd88af1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl.dataset.dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabcf2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path, header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "path = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/ionosphere.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(34)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d040c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba647ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl.dataset.dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = BCELoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b63b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b10ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aad032",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ecf4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv('ff.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=x['SP_imag'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555205de",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ed10de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test =  data.drop(['tan_del','epsilon'],axis=1).values, data.drop(['frequency','SP_real','SP_imag'],axis=1).values\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CSVDataset('https://raw.githubusercontent.com/jbrownlee/Datasets/master/housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c6dabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ad5587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0637635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402a87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f4f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbacc5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef027cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7d1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from network import Net\n",
    "from torch.utils.data import dataset\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e09d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self,file_name):\n",
    "        file_out=pd.read_csv(file_name)\n",
    "        x=file_out.drop(['tan_del','epsilon','index'],axis=1).values\n",
    "        y=file_out.drop(['frequency','SP_real','SP_imag','index','epsilon'],axis=1)\n",
    "        \n",
    "        sc=StandardScaler()\n",
    "        \n",
    "        x_train=sc.fit_transform(x)\n",
    "        y_train=y\n",
    "        \n",
    "        slf.X_train=torch.sensor(x_train,dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(y_train)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(sel.y_train)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X_train[idx],self.y_train[idx]\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99f6b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv ('ff.csv',index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=torch.utils.data.DataLoader(data,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed08768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5774f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pythonnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287dc29e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b79358",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['tan_del','epsilon','index'],axis=1).values, dataset.drop(['frequency','SP_real','SP_imag','index','epsilon'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59f26cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51decdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108398ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68be5c00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b9976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70818775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61958dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann_viz(model, view=True, filename=\"network.gv\"):\n",
    "    \"\"\"Vizualizez a Sequential model.\n",
    "\n",
    "    # Arguments\n",
    "        model: A Keras model instance.\n",
    "\n",
    "        view: whether to display the model after generation.\n",
    "\n",
    "        filename: where to save the vizualization. (a .gv file)\n",
    "\n",
    "        title: A title for the graph\n",
    "    \"\"\"\n",
    "    from graphviz import Digraph\n",
    "\n",
    "    input_layer = 0\n",
    "    hidden_layers_nr = 0\n",
    "    layer_types = []\n",
    "    hidden_layers = []\n",
    "    output_layer = 0\n",
    "    layers = [layer for layer in model.modules() if type(layer) == torch.nn.Linear]\n",
    "\n",
    "    for layer in layers:\n",
    "        if layer == layers[0]:\n",
    "            input_layer = layer.in_features\n",
    "            hidden_layers_nr += 1\n",
    "            if type(layer) == torch.nn.Linear:                \n",
    "                hidden_layers.append(layer.out_features)\n",
    "                layer_types.append(\"Dense\")\n",
    "            else:\n",
    "                raise Exception(\"Input error\")\n",
    "\n",
    "        else:\n",
    "            if layer == layers[-1]:\n",
    "                output_layer = layer.out_features\n",
    "            else:\n",
    "                hidden_layers_nr += 1\n",
    "                if type(layer) == torch.nn.Linear:\n",
    "\n",
    "                    hidden_layers.append(layer.out_features)\n",
    "                    layer_types.append(\"Dense\")\n",
    "                else:\n",
    "                    raise Exception(\"Hidden error\")\n",
    "        last_layer_nodes = input_layer\n",
    "        nodes_up = input_layer\n",
    "\n",
    "    g = Digraph(\"g\", filename=filename)\n",
    "    n = 0\n",
    "    g.graph_attr.update(splines=\"false\", nodesep=\"0.5\", ranksep=\"0\", rankdir='LR')\n",
    "    # Input Layer\n",
    "    with g.subgraph(name=\"cluster_input\") as c:\n",
    "        if type(layers[0]) == torch.nn.Linear:\n",
    "            the_label = \"Input Layer\"\n",
    "            if layers[0].in_features > 10:\n",
    "                the_label += \" (+\" + str(layers[0].in_features - 10) + \")\"\n",
    "                input_layer = 10\n",
    "            c.attr(color=\"white\")\n",
    "            for i in range(0, input_layer):\n",
    "                n += 1\n",
    "                c.node(str(n))\n",
    "                c.attr(labeljust=\"1\")\n",
    "                c.attr(label=the_label, labelloc=\"bottom\")\n",
    "                c.attr(rank=\"same\")                \n",
    "                c.node_attr.update(\n",
    "                    width=\"0.65\",\n",
    "                    style=\"filled\",                    \n",
    "                    shape=\"circle\",\n",
    "                    color=HAPPY_COLORS_PALETTE[3],\n",
    "                    fontcolor=HAPPY_COLORS_PALETTE[3],\n",
    "                )\n",
    "    for i in range(0, hidden_layers_nr):\n",
    "        with g.subgraph(name=\"cluster_\" + str(i + 1)) as c:\n",
    "            if layer_types[i] == \"Dense\":\n",
    "                c.attr(color=\"white\")\n",
    "                c.attr(rank=\"same\")\n",
    "                the_label = f'Hidden Layer {i + 1}'\n",
    "                if layers[i].out_features > 10:\n",
    "                    the_label += \" (+\" + str(layers[i].out_features - 10) + \")\"\n",
    "                    hidden_layers[i] = 10\n",
    "                c.attr(labeljust=\"right\", labelloc=\"b\", label=the_label)\n",
    "                for j in range(0, hidden_layers[i]):\n",
    "                    n += 1\n",
    "                    c.node(\n",
    "                        str(n),\n",
    "                        width=\"0.65\",\n",
    "                        shape=\"circle\",\n",
    "                        style=\"filled\",\n",
    "                        color=HAPPY_COLORS_PALETTE[0],\n",
    "                        fontcolor=HAPPY_COLORS_PALETTE[0],\n",
    "                    )\n",
    "                    for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n",
    "                        g.edge(str(h), str(n))\n",
    "                last_layer_nodes = hidden_layers[i]\n",
    "                nodes_up += hidden_layers[i]\n",
    "            else:\n",
    "                raise Exception(\"Hidden layer type not supported\")\n",
    "\n",
    "    with g.subgraph(name=\"cluster_output\") as c:\n",
    "        if type(layers[-1]) == torch.nn.Linear:\n",
    "            c.attr(color=\"white\")\n",
    "            c.attr(rank=\"same\")\n",
    "            c.attr(labeljust=\"1\")\n",
    "            for i in range(1, output_layer + 1):\n",
    "                n += 1\n",
    "                c.node(\n",
    "                    str(n),\n",
    "                    width=\"0.65\",\n",
    "                    shape=\"circle\",\n",
    "                    style=\"filled\",\n",
    "                    color=HAPPY_COLORS_PALETTE[4],\n",
    "                    fontcolor=HAPPY_COLORS_PALETTE[4],\n",
    "                    \n",
    "                )\n",
    "                for h in range(nodes_up - last_layer_nodes + 1, nodes_up + 1):\n",
    "                    g.edge(str(h), str(n))\n",
    "            c.attr(label=\"Output Layer\", labelloc=\"bottom\")\n",
    "            c.node_attr.update(\n",
    "                color=\"#2ecc71\", style=\"filled\", fontcolor=\"#2ecc71\", shape=\"circle\"\n",
    "            )\n",
    "\n",
    "    g.attr(arrowShape=\"none\")\n",
    "    g.edge_attr.update(arrowhead=\"none\", color=\"#707070\", penwidth=\"2\")\n",
    "    if view is True:\n",
    "        g.view()\n",
    "\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b09586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bd92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv ('ff.csv',index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['SP_imag', 'SP_real', 'frequency']]\n",
    "y = df[['epsilon']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b476387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7eeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.to_numpy()).float()\n",
    "y_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n",
    "\n",
    "X_test = torch.from_numpy(X_test.to_numpy()).float()\n",
    "y_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, 5)\n",
    "        self.fc2 = nn.Linear(5, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29637d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(X_train.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7861c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159b4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b97d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd01d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed72771a",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to(device)\n",
    "\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5976a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_pred):\n",
    "    predicted = y_pred.ge(.5).view(-1)\n",
    "    return (y_true == predicted).sum().float() / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056f0125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97742a97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def round_tensor(t, decimal_places=3):\n",
    "    return round(t.item(), decimal_places)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    y_pred = net(X_train)\n",
    "    \n",
    "    y_pred = torch.squeeze(y_pred)\n",
    "    train_loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        train_acc = calculate_accuracy(y_train, y_pred)\n",
    "\n",
    "        y_test_pred = net(X_test)\n",
    "        y_test_pred = torch.squeeze(y_test_pred)\n",
    "\n",
    "        test_loss = criterion(y_test_pred, y_test)\n",
    "\n",
    "        test_acc = calculate_accuracy(y_test, y_test_pred)\n",
    "        print(\n",
    "                f'''epoch {epoch}\n",
    "                Train set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}\n",
    "                Test  set - loss: {round_tensor(test_loss)}, accuracy: {round_tensor(test_acc)}\n",
    "                ''')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    train_loss.backward()\n",
    "    \n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7acfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0364ae06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd3f78c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2443ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AliReza\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (0,1,2,3,4,5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d60ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.332625031</td>\n",
       "      <td>-0.22542052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>8.00399971</td>\n",
       "      <td>-0.341092855</td>\n",
       "      <td>-0.2078062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>8.008000374</td>\n",
       "      <td>-0.348622531</td>\n",
       "      <td>-0.189921886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>8.012000084</td>\n",
       "      <td>-0.355208993</td>\n",
       "      <td>-0.171817496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26</td>\n",
       "      <td>8.015999794</td>\n",
       "      <td>-0.360849917</td>\n",
       "      <td>-0.153542429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814810</th>\n",
       "      <td>829605</td>\n",
       "      <td>11.984</td>\n",
       "      <td>0.395022</td>\n",
       "      <td>-0.019654</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814811</th>\n",
       "      <td>829606</td>\n",
       "      <td>11.988</td>\n",
       "      <td>0.393918</td>\n",
       "      <td>-0.032102</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814812</th>\n",
       "      <td>829607</td>\n",
       "      <td>11.992</td>\n",
       "      <td>0.392425</td>\n",
       "      <td>-0.044496</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814813</th>\n",
       "      <td>829608</td>\n",
       "      <td>11.996</td>\n",
       "      <td>0.390545</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814814</th>\n",
       "      <td>829609</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.388281</td>\n",
       "      <td>-0.069079</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>814814 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0            1             2             3    4    5\n",
       "1           22          8.0  -0.332625031   -0.22542052  0.0  1.0\n",
       "2           23   8.00399971  -0.341092855    -0.2078062  0.0  1.0\n",
       "3           24  8.008000374  -0.348622531  -0.189921886  0.0  1.0\n",
       "4           25  8.012000084  -0.355208993  -0.171817496  0.0  1.0\n",
       "5           26  8.015999794  -0.360849917  -0.153542429  0.0  1.0\n",
       "...        ...          ...           ...           ...  ...  ...\n",
       "814810  829605       11.984      0.395022     -0.019654  0.5  4.0\n",
       "814811  829606       11.988      0.393918     -0.032102  0.5  4.0\n",
       "814812  829607       11.992      0.392425     -0.044496  0.5  4.0\n",
       "814813  829608       11.996      0.390545     -0.056826  0.5  4.0\n",
       "814814  829609         12.0      0.388281     -0.069079  0.5  4.0\n",
       "\n",
       "[814814 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42d112a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>-0.332625031</td>\n",
       "      <td>-0.22542052</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.00399971</td>\n",
       "      <td>-0.341092855</td>\n",
       "      <td>-0.2078062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.008000374</td>\n",
       "      <td>-0.348622531</td>\n",
       "      <td>-0.189921886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.012000084</td>\n",
       "      <td>-0.355208993</td>\n",
       "      <td>-0.171817496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8.015999794</td>\n",
       "      <td>-0.360849917</td>\n",
       "      <td>-0.153542429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814810</th>\n",
       "      <td>11.984</td>\n",
       "      <td>0.395022</td>\n",
       "      <td>-0.019654</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814811</th>\n",
       "      <td>11.988</td>\n",
       "      <td>0.393918</td>\n",
       "      <td>-0.032102</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814812</th>\n",
       "      <td>11.992</td>\n",
       "      <td>0.392425</td>\n",
       "      <td>-0.044496</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814813</th>\n",
       "      <td>11.996</td>\n",
       "      <td>0.390545</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814814</th>\n",
       "      <td>12.0</td>\n",
       "      <td>0.388281</td>\n",
       "      <td>-0.069079</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>814814 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1             2             3    4    5\n",
       "1               8.0  -0.332625031   -0.22542052  0.0  1.0\n",
       "2        8.00399971  -0.341092855    -0.2078062  0.0  1.0\n",
       "3       8.008000374  -0.348622531  -0.189921886  0.0  1.0\n",
       "4       8.012000084  -0.355208993  -0.171817496  0.0  1.0\n",
       "5       8.015999794  -0.360849917  -0.153542429  0.0  1.0\n",
       "...             ...           ...           ...  ...  ...\n",
       "814810       11.984      0.395022     -0.019654  0.5  4.0\n",
       "814811       11.988      0.393918     -0.032102  0.5  4.0\n",
       "814812       11.992      0.392425     -0.044496  0.5  4.0\n",
       "814813       11.996      0.390545     -0.056826  0.5  4.0\n",
       "814814         12.0      0.388281     -0.069079  0.5  4.0\n",
       "\n",
       "[814814 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9316212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee2.to_csv('nnnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('nnnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd7047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5b2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5259ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4943bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path)\n",
    "        print(df)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, 1:4]\n",
    "        print(self.X)\n",
    "        self.y = df.values[:, 4]\n",
    "        print(self.y)\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act3 = Sigmoid()\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(100):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate accuracy\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "path = 'nnnn.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(3)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Accuracy: %.3f' % acc)\n",
    "# make a single prediction (expect class=1)\n",
    "row = [8,-0.332625031,-0.22542052]\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('fuck89')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21b07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [11.99199963 , 0.392425 ,  -0.04449595]\n",
    "\n",
    "yhat = predict(row, model)\n",
    "\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0a11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52dc520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.values[:,4]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f4d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08924bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be95a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de79a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for regression\n",
    "from numpy import vstack\n",
    "from numpy import sqrt\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "# dataset definition\n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path,header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, 1:4].astype('float32')\n",
    "        print(self.X)\n",
    "        self.y = df.values[:, 4].astype('float32')\n",
    "        print(self.y)\n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.03):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "# model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 10)\n",
    "        xavier_uniform_(self.hidden1.weight)\n",
    "        self.act1 = Sigmoid()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(10, 8)\n",
    "        xavier_uniform_(self.hidden2.weight)\n",
    "        self.act2 = Sigmoid()\n",
    "        # third hidden layer and output\n",
    "        self.hidden3 = Linear(8, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "\n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # third hidden layer and output\n",
    "        X = self.hidden3(X)\n",
    "        return X\n",
    "\n",
    "# prepare the dataset\n",
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl\n",
    "\n",
    "# train the model\n",
    "def train_model(train_dl, model):\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(3):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "# evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate mse\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    return mse\n",
    "\n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    "\n",
    "# prepare the data\n",
    "path = 'nnnn.csv'\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "# define the network\n",
    "model = MLP(3)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "# evaluate the model\n",
    "mse = evaluate_model(test_dl, model)\n",
    "print('MSE: %.3f, RMSE: %.3f' % (mse, sqrt(mse)))\n",
    "# make a single prediction (expect class=1)\n",
    "row = [11.99199963 , 0.392425 ,  -0.04449595]\n",
    "\n",
    "yhat = predict(row, model)\n",
    "print('Predicted: %.3f' % yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df416254",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [8.       ,  -0.33262503, -0.22542052]\n",
    "\n",
    "yhat = predict(row, model)\n",
    "\n",
    "print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffdc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4321615d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322f197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69f03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf34bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path):\n",
    "    # load the dataset\n",
    "    dataset = CSVDataset(path)\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, test_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        df = read_csv(path,header=None)\n",
    "        # store the inputs and outputs\n",
    "        self.X = df.values[:, 1:4].astype('float32')\n",
    "        print(self.X)\n",
    "        self.y = df.values[:, 4].astype('float32')\n",
    "        print(self.y)\n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.03):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626272b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and \n",
    "        assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 1000000, 3,10, 1\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.from_numpy(df.values[:, 1:4].astype('float32'))\n",
    "y =  torch.from_numpy(df.values[:, 4].astype('float32'))\n",
    "y = y.reshape((len(y), 1))\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to \n",
    "model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228bb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(np.asarray(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05324364",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eca76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and \n",
    "        assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary (differentiable) operations on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Construct our model by instantiating the class defined above.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to \n",
    "model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d9dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5990b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = tensor.[8.       ,  -0.33262503, -0.22542052]\n",
    "model(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00145611",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee=pd.read_csv('ff.csv',header=None,index_col=False)\n",
    "ee1=ee.drop([0],axis=1)\n",
    "ee2=ee1.drop(0)\n",
    "ee2\n",
    "ee2.to_csv('nnnn.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f5e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1cad06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20181b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.000</td>\n",
       "      <td>-0.332625</td>\n",
       "      <td>-0.225421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.004</td>\n",
       "      <td>-0.341093</td>\n",
       "      <td>-0.207806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.008</td>\n",
       "      <td>-0.348623</td>\n",
       "      <td>-0.189922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.012</td>\n",
       "      <td>-0.355209</td>\n",
       "      <td>-0.171817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.016</td>\n",
       "      <td>-0.360850</td>\n",
       "      <td>-0.153542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814809</th>\n",
       "      <td>11.984</td>\n",
       "      <td>0.395022</td>\n",
       "      <td>-0.019654</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814810</th>\n",
       "      <td>11.988</td>\n",
       "      <td>0.393918</td>\n",
       "      <td>-0.032102</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814811</th>\n",
       "      <td>11.992</td>\n",
       "      <td>0.392425</td>\n",
       "      <td>-0.044496</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814812</th>\n",
       "      <td>11.996</td>\n",
       "      <td>0.390545</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814813</th>\n",
       "      <td>12.000</td>\n",
       "      <td>0.388281</td>\n",
       "      <td>-0.069079</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>814814 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3    4    5\n",
       "0        8.000 -0.332625 -0.225421  0.0  1.0\n",
       "1        8.004 -0.341093 -0.207806  0.0  1.0\n",
       "2        8.008 -0.348623 -0.189922  0.0  1.0\n",
       "3        8.012 -0.355209 -0.171817  0.0  1.0\n",
       "4        8.016 -0.360850 -0.153542  0.0  1.0\n",
       "...        ...       ...       ...  ...  ...\n",
       "814809  11.984  0.395022 -0.019654  0.5  4.0\n",
       "814810  11.988  0.393918 -0.032102  0.5  4.0\n",
       "814811  11.992  0.392425 -0.044496  0.5  4.0\n",
       "814812  11.996  0.390545 -0.056826  0.5  4.0\n",
       "814813  12.000  0.388281 -0.069079  0.5  4.0\n",
       "\n",
       "[814814 rows x 5 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=pd.read_csv('nnnn.csv',header=None,index_col=False)\n",
    "data2=data1.drop([0],axis=1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5b407cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs=data2.values[:, 0:3]\n",
    "targets=data2.values[:, 4]\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f37d28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 4., 4., 4.], dtype=torch.float64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa460766",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "dataset = TensorDataset(inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "87166263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 20\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1d790513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.0840,  0.3205,  0.4259],\n",
      "        [ 9.1640,  0.2488, -0.6248],\n",
      "        [10.8960, -0.4189,  0.3028],\n",
      "        [10.3560,  0.4368,  0.3674],\n",
      "        [11.3960, -0.0502, -0.4842],\n",
      "        [11.6320, -0.5715,  0.1377],\n",
      "        [ 8.1960, -0.3953,  0.4249],\n",
      "        [ 9.7880,  0.4839, -0.2637],\n",
      "        [ 9.3120, -0.4805, -0.3233],\n",
      "        [10.5240,  0.4814, -0.3724],\n",
      "        [10.8680, -0.5645, -0.1593],\n",
      "        [10.4320,  0.5204,  0.0944],\n",
      "        [11.9640,  0.4375,  0.0201],\n",
      "        [10.6840, -0.2293, -0.4776],\n",
      "        [10.6320, -0.0168, -0.5258],\n",
      "        [11.0160,  0.0165,  0.5036],\n",
      "        [ 9.8360,  0.3267, -0.4929],\n",
      "        [ 8.8680, -0.2049,  0.4935],\n",
      "        [ 9.1920,  0.1215, -0.5237],\n",
      "        [ 8.4920,  0.5194, -0.4366]], dtype=torch.float64)\n",
      "tensor([3.2000, 2.2000, 2.2000, 1.8000, 3.4000, 2.8000, 3.8000, 3.2000, 2.8000,\n",
      "        1.6000, 3.0000, 2.8000, 3.4000, 1.4000, 2.0000, 2.2000, 3.8000, 1.2000,\n",
      "        1.6000, 2.2000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for inp,target in train_loader:\n",
    "    print(inp)\n",
    "    print(target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b6f03e1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ones' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/252545199.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ones' is not defined"
     ]
    }
   ],
   "source": [
    "cd=(0.01*ones(1, 3, requires_grad=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "58728a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.]], requires_grad=True)\n",
      "tensor([1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(1, 3, requires_grad=True)\n",
    "b = torch.ones(1, requires_grad=True)\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "3a502406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X):\n",
    "    return X @ w.t() + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d5d9a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :n tensor([[11.1396],\n",
      "        [10.6184],\n",
      "        [11.3816],\n",
      "        [ 8.9063],\n",
      "        [10.0347],\n",
      "        [ 8.1575],\n",
      "        [11.8533],\n",
      "        [ 9.8219],\n",
      "        [ 8.3865],\n",
      "        [ 9.0352],\n",
      "        [10.4591],\n",
      "        [10.2508],\n",
      "        [11.9788],\n",
      "        [12.0085],\n",
      "        [12.6649],\n",
      "        [10.5177],\n",
      "        [12.3980],\n",
      "        [13.0894],\n",
      "        [13.5073],\n",
      "        [11.0083]], grad_fn=<AddBackward0>)\n",
      "nActual targets is :n tensor([4.0000, 1.0000, 2.2000, 1.8000, 1.6000, 3.0000, 2.0000, 1.8000, 3.6000,\n",
      "        2.0000, 2.8000, 2.8000, 2.2000, 3.8000, 3.6000, 1.0000, 2.4000, 2.2000,\n",
      "        1.8000, 1.6000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x.float())\n",
    "    print(\"Prediction is :n\",preds)\n",
    "    print(\"nActual targets is :n\",y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0287fd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(predictions, targets):\n",
    "    difference = predictions - targets\n",
    "    return torch.sum(difference * difference)/ difference.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "177882ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50: Loss: 1.0895110980864893\n",
      "Epoch 1/50: Loss: 1.323842732455543\n",
      "Epoch 2/50: Loss: 1.052124513096005\n",
      "Epoch 3/50: Loss: 1.3638195781016367\n",
      "Epoch 4/50: Loss: 1.226634356773366\n",
      "Epoch 5/50: Loss: 0.9486033431298835\n",
      "Epoch 6/50: Loss: 1.1413187505912834\n",
      "Epoch 7/50: Loss: 0.9216775698030916\n",
      "Epoch 8/50: Loss: 0.8338946496032879\n",
      "Epoch 9/50: Loss: 0.7058347065155185\n",
      "Epoch 10/50: Loss: 0.6304994866266972\n",
      "Epoch 11/50: Loss: 0.9333842477909444\n",
      "Epoch 12/50: Loss: 0.9840117399303027\n",
      "Epoch 13/50: Loss: 0.6665203502822895\n",
      "Epoch 14/50: Loss: 1.0180778577513394\n",
      "Epoch 15/50: Loss: 0.7099039890589903\n",
      "Epoch 16/50: Loss: 0.7003848161567381\n",
      "Epoch 17/50: Loss: 0.9917188379603236\n",
      "Epoch 18/50: Loss: 0.7774255077542818\n",
      "Epoch 19/50: Loss: 0.8974366085654812\n",
      "Epoch 20/50: Loss: 0.9100825448043631\n",
      "Epoch 21/50: Loss: 0.7166800590634763\n",
      "Epoch 22/50: Loss: 1.0338812392572716\n",
      "Epoch 23/50: Loss: 0.7306500000191907\n",
      "Epoch 24/50: Loss: 0.7435212930059598\n",
      "Epoch 25/50: Loss: 1.0336924139843044\n",
      "Epoch 26/50: Loss: 0.7570555585707983\n",
      "Epoch 27/50: Loss: 0.8604248145765184\n",
      "Epoch 28/50: Loss: 0.7566958570818343\n",
      "Epoch 29/50: Loss: 1.1625692962120504\n",
      "Epoch 30/50: Loss: 0.8025959225471215\n",
      "Epoch 31/50: Loss: 0.8847256505747502\n",
      "Epoch 32/50: Loss: 0.39955689832412516\n",
      "Epoch 33/50: Loss: 0.732234671573339\n",
      "Epoch 34/50: Loss: 0.7524790034232963\n",
      "Epoch 35/50: Loss: 1.0732618565888996\n",
      "Epoch 36/50: Loss: 0.8467754308897937\n",
      "Epoch 37/50: Loss: 1.1442629550788554\n",
      "Epoch 38/50: Loss: 0.7478018843536779\n",
      "Epoch 39/50: Loss: 0.6606711836447728\n",
      "Epoch 40/50: Loss: 1.2382679775718868\n",
      "Epoch 41/50: Loss: 0.4785373551438279\n",
      "Epoch 42/50: Loss: 0.6981516445476743\n",
      "Epoch 43/50: Loss: 0.72684267776132\n",
      "Epoch 44/50: Loss: 0.8832331739920558\n",
      "Epoch 45/50: Loss: 0.4710080283052345\n",
      "Epoch 46/50: Loss: 0.830508036037324\n",
      "Epoch 47/50: Loss: 0.9899451141802607\n",
      "Epoch 48/50: Loss: 1.0043507809164156\n",
      "Epoch 49/50: Loss: 0.8388983320082559\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    # Iterate through training dataloader\n",
    "    for x,y in train_loader:\n",
    "        # Generate Prediction\n",
    "        preds = model(x.float())\n",
    "        # Get the loss and perform backpropagation\n",
    "        loss = mse_loss(preds, y)\n",
    "        loss.backward()\n",
    "        # Let's update the weights\n",
    "        with torch.no_grad():\n",
    "            w -= w.grad *1e-5\n",
    "            b -= b.grad * 1e-5\n",
    "            # Set the gradients to zero\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "        \n",
    "    print(f\"Epoch {i}/{epochs}: Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1af37748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :n tensor([[2.3295],\n",
      "        [2.3404],\n",
      "        [2.5096],\n",
      "        [2.5076],\n",
      "        [2.5432],\n",
      "        [2.5278],\n",
      "        [2.6184],\n",
      "        [2.4423],\n",
      "        [2.5063],\n",
      "        [2.4469],\n",
      "        [2.3813],\n",
      "        [2.5100],\n",
      "        [2.4845],\n",
      "        [2.6174],\n",
      "        [2.6449],\n",
      "        [2.4911],\n",
      "        [2.5945],\n",
      "        [2.3856],\n",
      "        [2.5116],\n",
      "        [2.5087]], grad_fn=<AddBackward0>)\n",
      "nActual targets is :n tensor([2.4000, 2.4000, 2.8000, 3.2000, 2.2000, 3.6000, 2.8000, 1.2000, 4.0000,\n",
      "        2.0000, 2.0000, 1.4000, 1.4000, 1.2000, 2.0000, 3.4000, 2.6000, 3.6000,\n",
      "        2.2000, 1.4000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x.float())\n",
    "    print(\"Prediction is :n\",preds)\n",
    "    print(\"nActual targets is :n\",y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "18821141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :n tensor([[2.3927],\n",
      "        [2.5668],\n",
      "        [2.0179]], grad_fn=<AddBackward0>)\n",
      "nActual targets is :n tensor([2.2000, 1.0000, 3.4000], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    preds = model(x.float())\n",
    "    print(\"Prediction is :n\",preds)\n",
    "    print(\"nActual targets is :n\",y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8db02b1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.1120,  0.4836, -0.4124],\n",
       "        [10.0680, -0.4742, -0.1171],\n",
       "        [ 8.1600, -0.5290,  0.2811]], dtype=torch.float64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a165c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d979f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760ede3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "69acaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d66f9698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "8b5af033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "011716a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv('nnnn.csv',header=None,index_col=False)\n",
    "data2=data1.drop([0],axis=1)\n",
    "data2\n",
    "\n",
    "X_train = data2.values[:,:3]\n",
    "y_train = data2.values[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6c9bc76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "sct = MinMaxScaler()\n",
    "X_train=sc.fit_transform(X_train.reshape(len(X_train),3))\n",
    "y_train =sct.fit_transform(y_train.reshape(len(y_train),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "04d6ac67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "550a17ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        ...,\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "2ba1ea8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999 nan\n",
      "1999 nan\n",
      "2999 nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/3048958872.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data1=pd.read_csv('nnnn.csv',header=None,index_col=False)\n",
    "data2=data1.drop([0],axis=1)\n",
    "data2\n",
    "\n",
    "x_train = data2.values[:,:3]\n",
    "y_train = data2.values[:,4]\n",
    "x_train = torch.from_numpy(x_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32).reshape(len(y_train),1))\n",
    "\n",
    "import torch\n",
    "\n",
    "\" Data for training\"\n",
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(3, 1),torch.nn.ReLU(),torch.nn.Linear(1, 1),)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(5000):\n",
    "    y_pred = model(x_train)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if t % 1000 == 999:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "y_pred_to_validate = model(x_test)\n",
    "print(y_pred_to_validate.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "e8c8872d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "725a069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(3,1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "188d597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(torch.nn.Linear(3, 1),torch.nn.ReLU(),torch.nn.Linear(1, 3),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "0372c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "l = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr =learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9991f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AliReza\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([814814])) that is different to the input size (torch.Size([814814, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (814814) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/582537510.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#calculate the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#backward propagation: calculate gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3109\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3111\u001b[1;33m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3112\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (814814) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "for epoch in range(num_epochs):\n",
    "    #forward feed\n",
    "    y_pred = model(X_train.requires_grad_())\n",
    "\n",
    "    #calculate the loss\n",
    "    loss= l(y_pred, y_train)\n",
    "\n",
    "    #backward propagation: calculate gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    #clear out the gradients from the last step loss.backward()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "1d2d31f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "18d8dd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [4.],\n",
       "        [4.],\n",
       "        [4.]])"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "a29c75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AliReza\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\Users\\AliReza\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([30, 1])) that is different to the input size (torch.Size([30, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import imageio\n",
    "\n",
    "torch.manual_seed(1)    # reproducible\n",
    "\n",
    "X_train = data2.values[:,:3]\n",
    "y_train = data2.values[:,4]\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "x = torch.unsqueeze(X_train,dim=1)  # x data (tensor), shape=(100, 1)\n",
    "y =torch.unsqueeze(y_train,dim=1)          # noisy y data (tensor), shape=(100, 1)\n",
    "\n",
    "# torch can only train on Variable, so convert them to Variable\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# another way to define a network\n",
    "net = torch.nn.Sequential(\n",
    "        torch.nn.Linear(3, 200),\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(200, 100),\n",
    "        torch.nn.LeakyReLU(),\n",
    "        torch.nn.Linear(100, 1),\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCH = 200\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x, y)\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    dataset=torch_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, num_workers=2,)\n",
    "\n",
    "# start training\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (batch_x, batch_y) in enumerate(loader): # for each training step\n",
    "        \n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "\n",
    "        prediction = net(b_x)     # input x and predict based on x\n",
    "\n",
    "        loss = loss_func(prediction, b_y)     # must be (1. nn output, 2. target)\n",
    "\n",
    "        optimizer.zero_grad()   # clear gradients for next train\n",
    "        loss.backward()         # backpropagation, compute gradients\n",
    "        optimizer.step()        # apply gradients\n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "bab72732",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=pd.read_csv('nnnn.csv',header=None,index_col=False)\n",
    "data2=data1.drop([0],axis=1)\n",
    "data2\n",
    "\n",
    "X_train = data2.values[:,:3]\n",
    "y_train = data2.values[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "de919e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #pytorch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "4d067903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "\n",
    "#X_train=mm.fit_transform(X_train.reshape(len(X_train),3))\n",
    "#y_train =ss.fit_transform(y_train.reshape(len(y_train),1))\n",
    "\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32).reshape(len(X_train),3))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32).reshape(len(X_train),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "17d186fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensors_final = torch.reshape(X_train,   (X_train.shape[0], 1, X_train.shape[1]))\n",
    "y_train_tensors = Variable(torch.Tensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "0e364dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        self.seq_length = seq_length #sequence length\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
    "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) #first Dense\n",
    "        out = self.relu(out) #relu\n",
    "        out = self.fc(out) #Final Output\n",
    "        return out\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "095cd260",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000 #1000 epochs\n",
    "learning_rate = 0.01 #0.001 lr\n",
    "\n",
    "input_size = 3 #number of features\n",
    "hidden_size = 2 #number of features in hidden state\n",
    "num_layers = 1 #number of stacked lstm layers\n",
    "\n",
    "num_classes = 1 #number of output classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "3a612da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "1c485bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "922c573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 7.46332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/1321570111.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#calculates the loss of the loss function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#improve from loss, i.e backprop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    outputs = lstm1.forward(X_train_tensors_final) #forward pass\n",
    "    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n",
    " \n",
    "  # obtain the loss function\n",
    "    loss = criterion(outputs, y_train_tensors)\n",
    " \n",
    "    loss.backward() #calculates the loss of the loss function\n",
    " \n",
    "    optimizer.step() #improve from loss, i.e backprop\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "61412a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0209],\n",
       "        [-0.0209],\n",
       "        [-0.0209],\n",
       "        ...,\n",
       "        [-0.0209],\n",
       "        [-0.0209],\n",
       "        [-0.0209]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "ded493fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        ...,\n",
       "        [4.],\n",
       "        [4.],\n",
       "        [4.]])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "a7cd593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(814814, 3)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 1, 32)             128       \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 4         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 231\n",
      "Trainable params: 231\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B8F953F4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B8F953F4C8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "15482/15482 [==============================] - 44s 3ms/step - loss: 1.2537\n",
      "Epoch 2/20\n",
      "15482/15482 [==============================] - 43s 3ms/step - loss: 0.8497\n",
      "Epoch 3/20\n",
      "15482/15482 [==============================] - 42s 3ms/step - loss: 0.8497\n",
      "Epoch 4/20\n",
      "15482/15482 [==============================] - 44s 3ms/step - loss: 0.8497\n",
      "Epoch 5/20\n",
      " 8254/15482 [==============>...............] - ETA: 19s - loss: 0.8497"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/229665601.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m#ypred = model.predict(xtest)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1206\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1207\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1208\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1209\u001b[0m             with tf.profiler.experimental.Trace(\n\u001b[0;32m   1210\u001b[0m                 \u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msteps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1248\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1250\u001b[1;33m       \u001b[0moriginal_spe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1251\u001b[0m       can_run_full_execution = (\n\u001b[0;32m   1252\u001b[0m           \u001b[0moriginal_spe\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    643\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m     raise NotImplementedError(\n\u001b[0;32m    647\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36mread_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;31m# Return an identity so it can get placed on whatever device the context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;31m# specifies instead of the device where the variable is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msparse_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1095\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1096\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1097\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4062\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4063\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 4064\u001b[1;33m         _ctx, \"Identity\", name, input)\n\u001b[0m\u001b[0;32m   4065\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4066\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x=X_train\n",
    "y=y_train\n",
    "x = x.reshape(x.shape[0], x.shape[1])\n",
    "y=y.reshape(y.shape[0], 1)\n",
    "print(x.shape)\n",
    "\n",
    "xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.05)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(32, 3, activation=\"relu\", input_shape=(3,1)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(3, activation=\"relu\"))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.summary()\n",
    "model.fit(xtrain, ytrain, batch_size=50,epochs=20, verbose=1)\n",
    "\n",
    "#ypred = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "4ed5c337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "814814"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a90b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "3a5f2553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate a sample dataset from random data: \n",
    "random.seed(123)\n",
    "def CreateDataset(N):\n",
    " a,b,c,y = [],[],[],[]\n",
    " for i in range(N):    \n",
    "  aa = i/10+random.uniform(-4,3)\n",
    "  bb = i/30+random.uniform(-4,4)\n",
    "  cc = i/40+random.uniform(-3,3)-5\n",
    "  yy = (aa+bb+cc/2)/3\n",
    "  a.append([aa])\n",
    "  b.append([bb])\n",
    "  c.append([cc])\n",
    "  y.append([yy])\n",
    " return np.hstack([a,b,c]), np.array(y)\n",
    "\n",
    "N = 150\n",
    "x,y = CreateDataset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "e60915fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.63345481, -3.30250658, -5.55654942],\n",
       "       [-3.14609836,  3.24292436, -7.746078  ],\n",
       "       [-0.04658572, -1.27575175, -2.83748029],\n",
       "       [-2.58236322, -1.20226674, -5.92222163],\n",
       "       [-1.88385653, -3.85330224, -5.28234524],\n",
       "       [-2.8867055 ,  0.94746238, -7.45573823],\n",
       "       [-1.19178718, -0.21342633, -2.41760809],\n",
       "       [-2.65064807, -2.62926615, -3.08456893],\n",
       "       [-3.05134818,  3.55587105, -4.35955149],\n",
       "       [-1.24274806,  3.00270115, -3.16223152],\n",
       "       [-0.59576489,  2.74530606, -6.50882518],\n",
       "       [ 1.36231768,  0.58326628, -2.86896885],\n",
       "       [-0.59315793, -0.41409479, -3.08432546],\n",
       "       [ 0.88926494,  1.91203108, -3.84488041],\n",
       "       [ 2.06660342,  3.58745523, -4.73318118],\n",
       "       [ 2.17929155, -0.50382695, -7.43395216],\n",
       "       [ 2.8792122 , -2.01494734, -2.67885889],\n",
       "       [ 4.09552138,  1.83125993, -6.55423867],\n",
       "       [ 0.15841307, -0.27045354, -7.11025533],\n",
       "       [ 1.09736949,  1.21868145, -5.5083634 ],\n",
       "       [-0.67656691, -2.62807497, -3.73009486],\n",
       "       [-1.81019624, -0.48469054, -6.10022141],\n",
       "       [ 1.92398243,  2.52264014, -1.99508035],\n",
       "       [ 2.77160587,  4.70101146, -1.64064497],\n",
       "       [ 2.80993497, -1.33301453, -4.4971311 ],\n",
       "       [-0.25687189,  4.76899552, -4.70800048],\n",
       "       [ 3.86155111, -2.78029449, -6.5416344 ],\n",
       "       [ 0.05223879, -1.04910122, -5.21271482],\n",
       "       [ 0.70350897,  0.407482  , -4.7778561 ],\n",
       "       [-0.33965416, -2.53246313, -5.14463132],\n",
       "       [-0.37847767,  0.46560251, -7.19863271],\n",
       "       [ 2.85060713, -2.37879472, -3.05976451],\n",
       "       [ 2.02945087,  4.94945207, -5.62475988],\n",
       "       [ 2.60757518,  3.10651953, -5.16601676],\n",
       "       [ 1.83856914,  2.24635683, -1.33188354],\n",
       "       [ 5.68716097,  0.51896922, -2.81869256],\n",
       "       [ 4.35336366,  0.17687695, -5.69021938],\n",
       "       [ 0.24863535,  4.26460483, -6.73204472],\n",
       "       [ 3.43087287,  1.56695769, -6.1731646 ],\n",
       "       [ 4.49152202,  4.62658555, -2.36820218],\n",
       "       [ 2.72481487, -1.654907  , -1.1504226 ],\n",
       "       [ 3.71972441,  2.40360767, -2.16134013],\n",
       "       [ 2.90372314, -2.10355388, -6.13390525],\n",
       "       [ 4.97912996,  0.22965082, -1.69853675],\n",
       "       [ 5.58115156,  5.31017984, -2.88699387],\n",
       "       [ 2.62664045,  3.82887326, -6.70545179],\n",
       "       [ 4.2511354 ,  4.50449068, -3.76168516],\n",
       "       [ 1.66160049,  1.56764675, -3.55225355],\n",
       "       [ 3.38300384,  4.74746848, -4.7595149 ],\n",
       "       [ 2.80225972,  1.68374523, -5.55536769],\n",
       "       [ 3.25440059,  0.85202811, -1.12042507],\n",
       "       [ 1.66253269, -0.72324105, -5.52800393],\n",
       "       [ 8.1668551 ,  4.85980295, -5.53502023],\n",
       "       [ 5.25237143, -1.5525331 , -4.77283078],\n",
       "       [ 3.78800972,  2.10145598, -1.07375407],\n",
       "       [ 7.16313602,  2.10680064, -1.90851967],\n",
       "       [ 4.98771326,  3.71114929, -1.93827335],\n",
       "       [ 3.49796217,  4.56221636, -3.10733496],\n",
       "       [ 5.20283971,  1.09983986, -5.20510116],\n",
       "       [ 5.40880287, -1.0244835 , -2.71630348],\n",
       "       [ 8.28787776,  4.32420824, -2.84505329],\n",
       "       [ 5.56132095,  3.59959073, -5.74110207],\n",
       "       [ 2.5670476 ,  4.95984325, -4.27895355],\n",
       "       [ 6.98263105,  1.46609336, -4.27297666],\n",
       "       [ 7.79200177,  0.87759959, -1.87771624],\n",
       "       [ 8.24334688, -0.38792491, -0.51161734],\n",
       "       [ 3.03709533,  1.51781884, -3.34715121],\n",
       "       [ 8.50550415, -0.36763724, -2.97081719],\n",
       "       [ 5.37962034,  5.64012942, -5.56603926],\n",
       "       [ 4.75791247,  4.85842069, -4.34159592],\n",
       "       [ 9.60954737, -1.25383533, -3.91949107],\n",
       "       [ 3.85226133,  2.95490924, -0.44625253],\n",
       "       [ 5.47286594,  3.5746867 , -4.60451408],\n",
       "       [ 6.37455163,  3.00550218, -4.68452015],\n",
       "       [ 8.23327789,  2.47463316, -0.71722083],\n",
       "       [ 5.42514891,  6.13883704, -5.03176499],\n",
       "       [ 3.73938557,  3.81497061, -1.69742489],\n",
       "       [ 7.04125229, -1.09125277, -0.08619462],\n",
       "       [ 4.7908623 ,  5.56373805, -0.70080422],\n",
       "       [ 5.44599904,  5.54484547, -3.24456208],\n",
       "       [10.29707769,  5.76685394, -1.35421533],\n",
       "       [ 6.12097169,  3.34083862, -0.08711866],\n",
       "       [ 6.44436356,  1.82534033, -1.82206415],\n",
       "       [ 4.73844951,  0.79356227, -4.91687775],\n",
       "       [ 7.31239697,  2.19981181, -2.25764871],\n",
       "       [ 7.97766782,  0.15148965, -5.59527357],\n",
       "       [ 8.27340539,  4.97752014, -0.28863659],\n",
       "       [10.57746827,  1.32202243, -0.63096054],\n",
       "       [ 8.07174081,  1.19290957, -5.24704763],\n",
       "       [ 7.99898691,  3.32635754, -4.09920378],\n",
       "       [ 6.36709177,  6.52712374, -5.34335049],\n",
       "       [11.6757458 ,  2.28463699, -0.97589326],\n",
       "       [ 5.97663634,  3.62672779, -1.12982265],\n",
       "       [ 8.3520821 ,  1.95172994, -2.71568714],\n",
       "       [ 6.7612194 ,  0.82637376, -4.20213083],\n",
       "       [ 5.7628966 ,  6.44685971, -5.18436047],\n",
       "       [12.08996684, -0.71235093, -1.6627193 ],\n",
       "       [ 9.52849342, -0.43193307,  0.07380064],\n",
       "       [10.91538483,  6.29971624, -2.93601696],\n",
       "       [ 7.93968218,  0.78742362, -4.84868304],\n",
       "       [10.69664896,  5.90770767, -4.22793334],\n",
       "       [ 6.28176405,  2.25892917, -4.55775195],\n",
       "       [ 7.77453784,  1.1763975 , -1.47547153],\n",
       "       [ 9.34222247,  3.91721708, -3.08447906],\n",
       "       [13.28569586,  3.44213369, -5.14443236],\n",
       "       [ 7.47455859,  5.34976683,  0.10076453],\n",
       "       [ 7.48185159,  3.21416846, -0.17709481],\n",
       "       [10.56260148, -0.29395279, -2.19976644],\n",
       "       [ 8.82129607,  1.30698758,  0.31970599],\n",
       "       [ 7.64008862,  6.27093721, -0.75694153],\n",
       "       [11.14976223,  2.26558056, -4.75337716],\n",
       "       [11.78121608,  7.37380597, -4.14622008],\n",
       "       [ 8.0830288 ,  3.53464662, -0.09185663],\n",
       "       [ 7.33720708,  6.07733626, -4.65716033],\n",
       "       [ 7.6260377 ,  0.97127348, -2.40436218],\n",
       "       [14.05872025,  5.12377839, -4.76724718],\n",
       "       [ 8.66188226,  3.5573434 , -4.55827589],\n",
       "       [12.37844726,  2.77146332,  0.66146156],\n",
       "       [13.53790373,  6.46859116, -3.63920179],\n",
       "       [10.91910596,  5.33935473, -4.48858159],\n",
       "       [ 8.06679028,  2.61905549, -2.59573054],\n",
       "       [13.60824628,  6.55055091, -4.62726919],\n",
       "       [ 8.65222065,  7.46488764, -3.22159214],\n",
       "       [ 9.17580091,  6.44098971, -0.21507191],\n",
       "       [12.63285514,  6.27269869,  0.25758654],\n",
       "       [14.47194845,  2.27276728, -2.34976781],\n",
       "       [15.49111596,  3.14195893, -1.09346823],\n",
       "       [14.62552199,  7.24781863, -1.20907736],\n",
       "       [11.24898709,  5.78852231, -4.78028077],\n",
       "       [11.35379664,  6.37081288, -0.89537575],\n",
       "       [ 9.55919449,  6.09028795, -1.08994335],\n",
       "       [12.21302742,  1.20361236, -1.14702102],\n",
       "       [14.4190347 ,  1.98980366, -0.89056669],\n",
       "       [ 9.79425938,  2.07607363, -2.52539699],\n",
       "       [14.84614416,  2.73306329,  0.69189657],\n",
       "       [16.38168546,  1.80684557, -4.46471104],\n",
       "       [14.70104487,  6.64831343,  0.80553346],\n",
       "       [14.40726171,  6.80746485, -2.38187941],\n",
       "       [10.50569326,  8.59930672, -0.92184032],\n",
       "       [11.79315238,  7.77543807, -4.08344723],\n",
       "       [10.39746721,  2.70547521, -3.71945294],\n",
       "       [12.97523374,  5.66862616, -1.35848293],\n",
       "       [14.0760163 ,  1.68835744, -1.2403918 ],\n",
       "       [17.1261282 ,  3.40498141,  0.37048452],\n",
       "       [14.86590197,  6.8964759 , -0.45231515],\n",
       "       [13.57655458,  2.71314981, -3.8135859 ],\n",
       "       [11.39271714,  1.67407637,  0.25409964],\n",
       "       [16.56119319,  7.44093486,  0.98378298],\n",
       "       [13.3113796 ,  6.32603837,  0.62167973],\n",
       "       [11.82612967,  0.96945386, -3.01706493]])"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebcd3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a028b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "c58bc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "data1=pd.read_csv('nnnn.csv',header=None,index_col=False)\n",
    "data2=data1.drop([0],axis=1)\n",
    "data2\n",
    "\n",
    "X_train = data2.values[:,:3]\n",
    "y_train = data2.values[:,4]\n",
    "x=X_train\n",
    "y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "a13101dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 4., 4., 4.])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "5be7e99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 128)               512       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 32)                4128      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 8)                 264       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,913\n",
      "Trainable params: 4,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def BuildModel():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=3,activation='relu')) \n",
    "    model.add(Dense(32, activation='relu')) \n",
    "    model.add(Dense(8,activation='relu')) \n",
    "    model.add(Dense(1,activation='linear'))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")   \n",
    "    return model\n",
    "\n",
    "BuildModel().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "34fcd728",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KerasPredict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/4124716623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKerasPredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBuildModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmse_krr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'KerasPredict' is not defined"
     ]
    }
   ],
   "source": [
    "regressor = KerasPredict(build_fn=BuildModel,nb_epoch=10,batch_size=50)\n",
    "regressor.fit(x,y) \n",
    "\n",
    "y_pred = regressor.predict(x)\n",
    "mse_krr = mean_squared_error(y, y_pred)\n",
    "print(mse_krr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "30dbf910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B90ADBF438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B90ADBF438> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "25463/25463 [==============================] - 79s 3ms/step - loss: 1.0928\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001B90ADBF798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_predict_function.<locals>.predict_function at 0x000001B90ADBF798> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "0.8498055910407807\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = BuildModel()\n",
    "model.fit(x, y,  verbose=1, shuffle=1)\n",
    "y_krm = model.predict(x)\n",
    "mse_krm=mean_squared_error(y, y_krm)\n",
    "print(mse_krm)\n",
    "\n",
    "plt.plot(y)\n",
    "plt.plot(y_krm, label=\"y-predicted\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "27284876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., ..., 4., 4., 4.]),\n",
       " array([[2.488148],\n",
       "        [2.488148],\n",
       "        [2.488148],\n",
       "        ...,\n",
       "        [2.488148],\n",
       "        [2.488148],\n",
       "        [2.488148]], dtype=float32))"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, y_krm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc6c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6138969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c38e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f6026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "d8625fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B9099F6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x000001B9099F6B88> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "25463/25463 [==============================] - 64s 2ms/step - loss: 0.0948\n",
      "Epoch 2/10\n",
      "15697/25463 [=================>............] - ETA: 25s - loss: 0.0945 ETA: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19516/3458929252.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SGD'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m# new instance where we do not know the answer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1214\u001b[0m                 _r=1):\n\u001b[0;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Desktop\\WEnvs\\py3.7\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example of making predictions for a regression problem\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "# generate regression dataset\n",
    "X, y = x,y\n",
    "\n",
    "# define and fit the final model\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=3, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss='mse', optimizer='SGD')\n",
    "model.fit(X, y, epochs=10, verbose=1)\n",
    "# new instance where we do not know the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "a50657e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.        , -0.33262503, -0.22542052],\n",
       "       [ 8.00399971, -0.34109285, -0.2078062 ],\n",
       "       [ 8.00800037, -0.34862253, -0.18992189],\n",
       "       ...,\n",
       "       [11.99199963,  0.392425  , -0.04449595],\n",
       "       [11.99600029,  0.39054498, -0.05682611],\n",
       "       [12.        ,  0.3882809 , -0.06907865]])"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "f0e12d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=[12.          0.3882809  -0.06907865], Predicted=[0.5010301]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Xnew = array([[12.        ,  0.3882809 , -0.06907865]])\n",
    "# make a prediction\n",
    "ynew = model.predict(Xnew)\n",
    "# show the inputs and predicted outputs\n",
    "print(\"X=%s, Predicted=%s\" % (Xnew[0], ynew[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "01db0de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.000</td>\n",
       "      <td>-0.332625</td>\n",
       "      <td>-0.225421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.004</td>\n",
       "      <td>-0.341093</td>\n",
       "      <td>-0.207806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.008</td>\n",
       "      <td>-0.348623</td>\n",
       "      <td>-0.189922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.012</td>\n",
       "      <td>-0.355209</td>\n",
       "      <td>-0.171817</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.016</td>\n",
       "      <td>-0.360850</td>\n",
       "      <td>-0.153542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814809</th>\n",
       "      <td>11.984</td>\n",
       "      <td>0.395022</td>\n",
       "      <td>-0.019654</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814810</th>\n",
       "      <td>11.988</td>\n",
       "      <td>0.393918</td>\n",
       "      <td>-0.032102</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814811</th>\n",
       "      <td>11.992</td>\n",
       "      <td>0.392425</td>\n",
       "      <td>-0.044496</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814812</th>\n",
       "      <td>11.996</td>\n",
       "      <td>0.390545</td>\n",
       "      <td>-0.056826</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814813</th>\n",
       "      <td>12.000</td>\n",
       "      <td>0.388281</td>\n",
       "      <td>-0.069079</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>814814 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1         2         3    4    5\n",
       "0        8.000 -0.332625 -0.225421  0.0  1.0\n",
       "1        8.004 -0.341093 -0.207806  0.0  1.0\n",
       "2        8.008 -0.348623 -0.189922  0.0  1.0\n",
       "3        8.012 -0.355209 -0.171817  0.0  1.0\n",
       "4        8.016 -0.360850 -0.153542  0.0  1.0\n",
       "...        ...       ...       ...  ...  ...\n",
       "814809  11.984  0.395022 -0.019654  0.5  4.0\n",
       "814810  11.988  0.393918 -0.032102  0.5  4.0\n",
       "814811  11.992  0.392425 -0.044496  0.5  4.0\n",
       "814812  11.996  0.390545 -0.056826  0.5  4.0\n",
       "814813  12.000  0.388281 -0.069079  0.5  4.0\n",
       "\n",
       "[814814 rows x 5 columns]"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bb776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa913f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7549c99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "744a2e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[len(y)-10:len(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "fb2f84bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 606,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "0b4a10bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 4., 4., 4.])"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data1=pd.read_csv('nnnn.csv',header=None,index_col=False)\n",
    "data2=data1.drop([0],axis=1)\n",
    "data2\n",
    "\n",
    "X_train = data2.values[:,:3]\n",
    "y_train = data2.values[:,4]\n",
    "y_train1 = data2.values[:,3]\n",
    "y1=y_train1\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "  \n",
    " # create regressor object\n",
    "regressor = RandomForestRegressor(n_estimators = 1000, random_state = 0)\n",
    "  \n",
    "# fit the regressor with x and y data\n",
    "regressor.fit(x, y)\n",
    "\n",
    "Xnew = array([[11.99199963,  0.392425  , -0.04449595]])\n",
    "Y_pred1 = regressor1.predict(Xnew)\n",
    "Y_pred = regressor.predict(Xnew)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "  \n",
    " # create regressor object\n",
    "regressor1 = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "  \n",
    "# fit the regressor with x and y data\n",
    "regressor1.fit(x, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "396bbdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "  \n",
    " # create regressor object\n",
    "regressor = RandomForestRegressor(n_estimators = 1000, random_state = 0)\n",
    "  \n",
    "# fit the regressor with x and y data\n",
    "regressor.fit(x, y)\n",
    "\n",
    "Xnew = array([[11.99199963,  0.392425  , -0.04449595]])\n",
    "Y_pred1 = regressor1.predict(Xnew)\n",
    "Y_pred = regressor.predict(Xnew)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "  \n",
    " # create regressor object\n",
    "regressor1 = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "  \n",
    "# fit the regressor with x and y data\n",
    "regressor1.fit(x, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73d287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "08ba50c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "6c8969ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.974]), array([0.4845]))"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred,Y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "0a6f069d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=0)"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "a130c939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "2a4b31c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0. , 0. , ..., 0.5, 0.5, 0.5])"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5738e787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
